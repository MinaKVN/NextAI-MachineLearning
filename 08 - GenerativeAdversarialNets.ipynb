{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribution: \n",
    "\n",
    "   * Most material is adapted from [Agustinus Kristiadi's blog](https://wiseodd.github.io/techblog/2017/01/20/gan-pytorch/) and [Generative models repository](https://github.com/wiseodd/generative-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Networks\n",
    "\n",
    "We're now doing to use PyTorch to implement a *vanilla* version of one of the most popular generative models today, the GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import os\n",
    "\n",
    "import torchvision\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = torchvision.datasets.MNIST(\n",
    "    root='./data/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalizes in the range [0.0, 1.0]\n",
    "    download=True,                                  # download it if you don't have it\n",
    ")\n",
    "\n",
    "mb_size = 64\n",
    "Z_dim = 100\n",
    "n_r = mnist_train.train_data.shape[1]\n",
    "n_c = mnist_train.train_data.shape[2]\n",
    "X_dim =  n_r * n_c\n",
    "h_dim = 128\n",
    "lr = 1e-3\n",
    "c = 0\n",
    "n_samples = mb_size # used for visualization during learning\n",
    "\n",
    "assert(n_samples <= mb_size)  # this is assumed below\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=mnist_train, batch_size=mb_size,\n",
    "                               shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meet the Generator and Discriminator\n",
    "\n",
    "A GAN has two crucial parts, the Generator $G(\\mathbf{z})$ (which we are ultimately interested in), and a Discriminator $D(\\mathbf{x})$ which is used to train the Generator and then discarded.\n",
    "\n",
    "Let's start by constructing $G(\\mathbf{z})$\n",
    "\n",
    "We are going to use a popular type of weight initialization commonly called \"Xavier\" initialization after the first author of this paper:\n",
    "\n",
    "    Glorot, Xavier, and Yoshua Bengio. 2010. “Understanding the Difficulty of\n",
    "    Training Deep Feedforward Neural Networks.” In Proceedings of the\n",
    "    Thirteenth International Conference on Artificial Intelligence and\n",
    "    Statistics, 249–56."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wzh = torch.empty(Z_dim, h_dim)\n",
    "nn.init.xavier_normal_(Wzh)\n",
    "Wzh = Variable(Wzh, requires_grad=True)\n",
    "bzh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Whx = torch.empty(h_dim, X_dim)\n",
    "nn.init.xavier_normal_(Whx)\n",
    "Whx = Variable(Whx, requires_grad=True)\n",
    "bhx = Variable(torch.zeros(X_dim), requires_grad=True)\n",
    "\n",
    "def G(z):\n",
    "    h = F.relu(z.mm(Wzh) + bzh)\n",
    "    X = torch.sigmoid(h.mm(Whx) + bhx)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence, the Generator takes a noise vector as input and maps it through a ReLU hidden layer to the image domain.\n",
    "\n",
    "Now, we define $D(\\mathbf{x})$ which is just a classifier as we have seen before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = torch.empty(X_dim, h_dim)\n",
    "nn.init.xavier_normal_(Wxh)\n",
    "Wxh = Variable(Wxh, requires_grad=True)\n",
    "bxh = Variable(torch.zeros(h_dim), requires_grad=True)\n",
    "\n",
    "Why = torch.empty(h_dim, 1)\n",
    "nn.init.xavier_normal_(Why)\n",
    "Why = Variable(Why, requires_grad=True)\n",
    "bhy = Variable(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "def D(X):\n",
    "    h = F.relu(X.mm(Wxh) + bxh)\n",
    "    y = F.sigmoid(h.mm(Why) + bhy)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each of these networks will be optimized according to their own criterion, we have deliberately kept them separate.\n",
    "\n",
    "This requires a little extra bookkeeping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_params = [Wzh, bzh, Whx, bhx]\n",
    "D_params = [Wxh, bxh, Why, bhy]\n",
    "params = G_params + D_params\n",
    "\n",
    "def reset_grad():\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            p.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set up the optimization procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_solver = optim.Adam(G_params, lr=lr)\n",
    "D_solver = optim.Adam(D_params, lr=lr)\n",
    "\n",
    "# these will be targets used to denote \"real\" or \"fake\" data\n",
    "ones_label = Variable(torch.ones(mb_size))\n",
    "zeros_label = Variable(torch.zeros(mb_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward and backward pass\n",
    "\n",
    "Now we will wrap up each of the Generator and Discriminator's forward and backward steps in a loop over mini-batches and epochs, but let's first describe the forward and backward pass on a single mini-batch.\n",
    "\n",
    "Let's grab a batch of data for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise as input to the Generator\n",
    "z = Variable(torch.randn(mb_size, Z_dim))\n",
    "\n",
    "# data as input to the discriminator\n",
    "X, _ = next(iter(train_loader))\n",
    "X = Variable(X.view(-1, X_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll show the Discriminator's forward and backward pass first.\n",
    "\n",
    "We pass some noise through the Generator to create some \"fake\" data. We assign labels that say \"fake\" to the \"fake\" data and labels that say \"real\" to the batch of data we took from the training set.\n",
    "\n",
    "Then the Discriminator tries to assign \"real\" to the real data and \"fake\" to the data coming from the Generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator forward-loss-backward-update\n",
    "G_sample = G(z)\n",
    "\n",
    "D_fake = D(G_sample)\n",
    "D_real = D(X)\n",
    "\n",
    "D_loss_real = F.binary_cross_entropy(D_real, ones_label.view(-1, 1))\n",
    "D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label.view(-1, 1))\n",
    "D_loss = D_loss_real + D_loss_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've computed the loss we can backprop through the Discriminator and update its parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss.backward()\n",
    "D_solver.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have two different optimizers, we need to clear the computed gradient in our computational graph as we do not need it anymore. This is important, because there will be a subsequent call of `backward()` on the Generator, and `D_solver` shares some subgraphs with `G_solver`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the forward and backward pass for the Generator.\n",
    "\n",
    "Again, we sample some noise, and pass it through the Generator to create some \"fake\" data.\n",
    "\n",
    "Now we apply it to the Discriminator. Except that from the Generator's perspective it wants to *fool* the Discriminator, so we apply \"real\" labels as ground truth.\n",
    "\n",
    "We backprop through the Generator, update its parameters, and clear the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = Variable(torch.randn(mb_size, Z_dim))\n",
    "G_sample = G(z)\n",
    "\n",
    "D_fake = D(G_sample)\n",
    "\n",
    "G_loss = F.binary_cross_entropy(D_fake, ones_label.view(-1,1))\n",
    "\n",
    "G_loss.backward()\n",
    "G_solver.step()\n",
    "\n",
    "reset_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. Notice that we re-sampled the noise before the Generator forward-backward step. Is this necessary?\n",
    "\n",
    "2. Notice that for the discriminator we summed `loss_real` and `loss_fake` before performing the backward pass and updated the parameters based on the gradient of their sum. What would happen if we did a backward pass and update for each of them separately?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it together in a learning loop\n",
    "\n",
    "Alright, let's put these steps together and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for it in range(100000):\n",
    "    # Sample data\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    \n",
    "    X, _ = next(iter(train_loader))\n",
    "    X = Variable(X.view(-1, X_dim))\n",
    "\n",
    "    # Dicriminator forward-loss-backward-update\n",
    "    G_sample = G(z)\n",
    "    D_real = D(X)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    D_loss_real = F.binary_cross_entropy(D_real, ones_label.view(-1, 1))\n",
    "    D_loss_fake = F.binary_cross_entropy(D_fake, zeros_label.view(-1, 1))\n",
    "    D_loss = D_loss_real + D_loss_fake\n",
    "\n",
    "    D_loss.backward()\n",
    "    D_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Generator forward-loss-backward-update\n",
    "    z = Variable(torch.randn(mb_size, Z_dim))\n",
    "    G_sample = G(z)\n",
    "    D_fake = D(G_sample)\n",
    "\n",
    "    G_loss = F.binary_cross_entropy(D_fake, ones_label.view(-1, 1))\n",
    "\n",
    "    G_loss.backward()\n",
    "    G_solver.step()\n",
    "\n",
    "    # Housekeeping - reset gradient\n",
    "    reset_grad()\n",
    "\n",
    "    # Print and plot every now and then\n",
    "    if it % 1000 == 0:\n",
    "        print('Iter-{}; D_loss: {}; G_loss: {}'.format(it, D_loss.data.numpy(), G_loss.data.numpy()))\n",
    "\n",
    "        samples = G(z).data.numpy()[:n_samples]\n",
    "\n",
    "        n_gs = int(np.sqrt(n_samples))\n",
    "        \n",
    "        fig = plt.figure(figsize=(4, 4))\n",
    "        gs = gridspec.GridSpec(n_gs, n_gs)\n",
    "        gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for i, sample in enumerate(samples):\n",
    "            ax = plt.subplot(gs[i])\n",
    "            plt.axis('off')\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_aspect('equal')\n",
    "            plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    "\n",
    "        if not os.path.exists('out/'):\n",
    "            os.makedirs('out/')\n",
    "\n",
    "        plt.savefig('out/{}.png'.format(str(c).zfill(3)), bbox_inches='tight')\n",
    "        c += 1\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "1. Do you notice any trends in the trajectory of `D_loss` and `G_loss` over learning?\n",
    "\n",
    "2. Wrap up the Generator and the Discriminator into their own classes, inheriting from `nn.Module`, similar to the way we did for the feedforward and convolutional neural networks. Does this make experimentation any more convenient?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
