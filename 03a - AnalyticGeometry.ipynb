{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZR1jKOx4JVys"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ogUJXZqJJLc3"
   },
   "source": [
    "# Analytic Geometry\n",
    "\n",
    "We've already seen multiple views of vectors:\n",
    "\n",
    "* as an array of numbers (a computer science “data structure” view)\n",
    "* as an arrow with a direction and magnitude (a physics view); and\n",
    "* as an object that obeys addition and scaling (a mathematics view)\n",
    "\n",
    "We're going to take a slightly more abstract take on the physics view and add some geometric interpretation and intuition to vectors, vector spaces, and linear mappings. This will yield some useful tools for machine learning topics such as regression, matrix decomposition, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVcDz2n8JR9P"
   },
   "source": [
    "## Norms\n",
    "\n",
    "Taking the square root of an inner product of a vector an itself gives a scalar $\\mathbf{x}^T \\mathbf{x}$ which is known as the Euclidean *norm*. It is a measure of the length of the vector $x$ and is written $||\\mathbf{x}||_2$. It is equivalent to taking the square root of the sum of the squares of the elements of the vector.\n",
    "\n",
    "Let's look at a few different ways to compute the squared Euclidean norm. We're employing the `%%time` cell magic to time execution of a cell. For more control over the measurement (e.g. using repeated loops) you can use `%%timeit`. For more information on profiling and timing code, check out [Jake VanderPlas' chapter on the subject](https://jakevdp.github.io/PythonDataScienceHandbook/01.07-timing-and-profiling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7gy24gfKAup"
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MVN8GKmBNRC_"
   },
   "source": [
    "Explicitly implementing the dot product ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uc-14sIfL9x2"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.sqrt(np.sum(x * x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Os8ccL3LNYkv"
   },
   "source": [
    "Making use of `numpy.dot`, which looks a bit more like the algebraic expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PakFPv2KOxS"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.sqrt(np.dot(x.T, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYbxsmYyNkYt"
   },
   "source": [
    "Note that we didn't actually need the transpose&mdash; NumPy automatically does dot product with two vector inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MOqzNpHNK9rn"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.sqrt(np.dot(x, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVE4QrP-OF1E"
   },
   "source": [
    "Finally, we use a more powerful function for computing general norms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGlvyoqSLCN_"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "np.linalg.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SVW1ZFEO_vt"
   },
   "source": [
    "If you're feeling really geeky, you can even call raw BLAS functions directly from SciPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNjsi6u1Ldm-"
   },
   "outputs": [],
   "source": [
    "import scipy.linalg\n",
    "%time nrm2, = scipy.linalg.get_blas_funcs(('nrm2',), (x,))\n",
    "print(nrm2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Od-74VE0Po6E"
   },
   "source": [
    "What is the difference between `numpy.linalg` and `scipy.linalg`? Well, according to the [SciPy docs](https://docs.scipy.org/doc/scipy/reference/tutorial/linalg.html):\n",
    "\n",
    "> A scipy.linalg contains all the functions that are in numpy.linalg. Additionally, scipy.linalg also has some other advanced functions that are not in numpy.linalg. Another advantage of using scipy.linalg over numpy.linalg is that it is always compiled with BLAS/LAPACK support, while for NumPy this is optional. Therefore, the SciPy version might be faster depending on how NumPy was installed.\n",
    "\n",
    "It is recommended therefore to use `scipy.linalg` instead of `numpy.linalg` unless you don't want to add `scipy` as a dependency to your `numpy` program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7b51YjDO1NI"
   },
   "source": [
    "### Manhattan ($\\ell_1$) norm\n",
    "\n",
    "The $\\ell_1$ norm for $\\mathbf{x} \\in \\mathbb{R}^n$, $||x||_1 = \\sum_{i=1}^n |x_i|$ is frequently encountered in machine learning. You can compute it similarly to the above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ReAsYS_rQc5Y"
   },
   "outputs": [],
   "source": [
    "np.linalg.norm(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWRwfe-5Rz8W"
   },
   "source": [
    "NumPy provides the `numpy.isclose` and `numpy.allclose` functions to test  array-like objects for equality up to desired tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5zTvqCV6RJIZ"
   },
   "outputs": [],
   "source": [
    "np.allclose(np.linalg.norm(x, 1), np.sum(np.abs(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PKsKExlMRddR"
   },
   "source": [
    "## Tests for symmetry and positive definiteness\n",
    "\n",
    "Since symmetric, positive definite matrices play an important role in machine learning, it is important to be able to test for these properties.\n",
    "\n",
    "Let's take the matrices from Example 3.4:\n",
    "\n",
    "$$\\mathbf{A}_1 = \n",
    "\\begin{bmatrix} 9 & 6\\\\ 6 & 5\\\\\\end{bmatrix}\n",
    "\\mathbf{A}_2 = \n",
    "\\begin{bmatrix} 9 & 6\\\\ 6 & 3\\\\\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lC5AeESpYvbx"
   },
   "outputs": [],
   "source": [
    "A1 = np.array([[9, 6], [6, 5]], dtype='float')\n",
    "A2 = np.array([[9, 6], [6, 3]], dtype='float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wZk7sUW8VNcX"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Before unfolding these blocks, why don't you see if you can write a function to test whether a matrix is symmetric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "LgcjvcIESpVg"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "from numpy import diag_indices_from, empty_like, finfo, sqrt, asanyarray\n",
    "from numpy.linalg import LinAlgError, cholesky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "34DsXsEKU6jm"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Source: https://numpy-sugar.readthedocs.io/en/stable/_modules/numpy_sugar/linalg/property.html#check_symmetry\n",
    "def check_symmetry(A):\n",
    "    \"\"\"Check if ``A`` is a symmetric matrix.\n",
    "\n",
    "    Args:\n",
    "        A (array_like): Matrix.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if ``A`` is symmetric; ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    A = asanyarray(A)\n",
    "    if A.ndim != 2:\n",
    "        raise ValueError(\"Checks symmetry only for bi-dimensional arrays.\")\n",
    "\n",
    "    if A.shape[0] != A.shape[1]:\n",
    "        return False\n",
    "\n",
    "    return abs(A - A.T).max() < sqrt(finfo(float).eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-WiclL7VAkB"
   },
   "outputs": [],
   "source": [
    "print(check_symmetry(A1))\n",
    "print(check_symmetry(A2))\n",
    "print(check_symmetry([[1, 2], [3, 4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhfg16k4V9y_"
   },
   "source": [
    "With positive definiteness, it's a little more nuanced. An efficient way to test this is to use the NumPy implementation for a particular kind of matrix decomposition called the Cholesky decomposition. We'll explore it in detail in the next unit, so for now let's just treat it like a black box. `numpy.cholesky` will through a `LinAlgError` if its argument is not positive definite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnMRXg5vWx-t"
   },
   "outputs": [],
   "source": [
    "# Source: https://numpy-sugar.readthedocs.io/en/stable/_modules/numpy_sugar/linalg/property.html#check_definite_positiveness\n",
    "def check_definite_positiveness(A):\n",
    "    \"\"\"Check if ``A`` is a definite positive matrix.\n",
    "\n",
    "    Args:\n",
    "        A (array_like): Matrix.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if ``A`` is definite positive; ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cholesky(A)\n",
    "    except LinAlgError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DUUpLsjgW16n"
   },
   "outputs": [],
   "source": [
    "print(check_definite_positiveness(A1))\n",
    "print(check_definite_positiveness(A2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jVeu0s6TXh9K"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Before unfolding the next block, see if you can modify the function above to test for positive semidefiniteness. This is tricky, so don't worry too much if you're stumped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "bEwTMQfbX_3I"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "# Source: https://numpy-sugar.readthedocs.io/en/stable/_modules/numpy_sugar/linalg/property.html#check_semidefinite_positiveness\n",
    "def check_semidefinite_positiveness(A):\n",
    "    \"\"\"Check if ``A`` is a positive semi-definite matrix.\n",
    "\n",
    "    Args:\n",
    "        A (array_like): Matrix.\n",
    "\n",
    "    Returns:\n",
    "        bool: ``True`` if ``A`` is positive semidefinite; ``False`` otherwise.\n",
    "    \"\"\"\n",
    "    B = empty_like(A)\n",
    "    B[:] = A\n",
    "    B[diag_indices_from(B)] += sqrt(finfo(float).eps)\n",
    "    try:\n",
    "        cholesky(B)\n",
    "    except LinAlgError:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8xFllnmYTdM"
   },
   "outputs": [],
   "source": [
    "print(check_semidefinite_positiveness(A1))\n",
    "print(check_semidefinite_positiveness(A2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c0bf8jygaDu5"
   },
   "source": [
    "## Orthogonality\n",
    "\n",
    "Let's consider the two vectors in Example 3.7, $\\mathbf{x} = [1, 1]^{\\top}$, $\\mathbf{y} = [-1, 1]^{\\top} \\in \\mathbb{R}^2$. \n",
    "\n",
    "Using the dot product as the inner product yields $\\mathbf{x} \\perp \\mathbf{y}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rkgxp3JzauTk"
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 1])\n",
    "y = np.array([-1, 1])\n",
    "\n",
    "np.dot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZPRE5wzTpmOM"
   },
   "source": [
    "However, if we choose the inner product\n",
    "\n",
    "$$ \n",
    "\\langle \\mathbf{x} , \\mathbf{y} \\rangle = \\mathbf{x}^{\\top}\n",
    "\\begin{bmatrix}\n",
    "2 & 0\\\\\n",
    "0 & 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\mathbf{y},\n",
    "$$\n",
    "\n",
    "we get that the cosine of the angle $\\omega$ between $\\mathbf{x}$ and $\\mathbf{y}$ given by\n",
    "\n",
    "$$\n",
    "\\cos \\omega = \\frac{\\langle \\mathbf{x} , \\mathbf{y} \\rangle}{\\|\\mathbf{x}\\|\\|\\mathbf{y}\\|}\n",
    "$$\n",
    "\n",
    "is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ed2gfOApqLn4"
   },
   "outputs": [],
   "source": [
    "A = np.array([[2, 0], [0, 1]])\n",
    "\n",
    "# define our inner product\n",
    "def innerprod(x, y, A):\n",
    "    return x.dot(A).dot(y)\n",
    "\n",
    "# norm is based on our new inner product\n",
    "def norm(x, A):\n",
    "    return np.sqrt(innerprod(x, x, A))\n",
    "  \n",
    "cos_omega = innerprod(x, y, A) / (norm(x, A) * norm(y, A))\n",
    "print(cos_omega)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nOE0Em6wACa"
   },
   "source": [
    "To get $\\omega$ we take the triginometric inverse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DHtE9-6rqwZu"
   },
   "outputs": [],
   "source": [
    "omega = np.arccos(cos_omega)\n",
    "print(omega)  # in radians\n",
    "print(np.rad2deg(omega))  # in degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J3r_pYr0ra_I"
   },
   "source": [
    "So we see that vectors that are orthogonal with respect to one inner product are not necessarily orthogonal to a different inner product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xQQK6o8zyOsU"
   },
   "source": [
    "### Orthogonal matrix\n",
    "\n",
    "A square matrix is orthogonal only if its columns are orthonomal.\n",
    "\n",
    "Let's consider the matrix\n",
    "\n",
    "$$\\mathbf{A} = \n",
    "\\begin{bmatrix} \\cos(0.5) & -\\sin(0.5)\\\\ \\sin(0.5) & \\cos(0.5) \\\\\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uz2jTjUbyZLN"
   },
   "outputs": [],
   "source": [
    "A = np.array([[np.cos(0.5), -np.sin(0.5)], [np.sin(0.5), np.cos(0.5)]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_s8_pOLTziGL"
   },
   "source": [
    "First, we'll check if its columns are orthogonal and unit norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6mMfOCboy3G2"
   },
   "outputs": [],
   "source": [
    "print(np.dot(A[:, 0], A[:, 1]))\n",
    "print(np.linalg.norm(A[:, 0]))\n",
    "print(np.linalg.norm(A[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNSUx9PVzhhg"
   },
   "source": [
    "For an orthogonal matrix $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$, $\\mathbf{A}\\mathbf{A}^{\\top} = \\mathbf{I}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EpT5J9b70QNo"
   },
   "outputs": [],
   "source": [
    "np.allclose(np.dot(A, A.T), np.eye(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CD4UOyN80n3I"
   },
   "source": [
    "The above property also implies that $\\mathbf{A}^{-1} = \\mathbf{A}^{\\top}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "htNi0lWp2MSP"
   },
   "outputs": [],
   "source": [
    "np.allclose(A.T, np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZyuX9MkE3Gbz"
   },
   "source": [
    "### Exercise\n",
    "\n",
    "We saw that the columns of an orthogonal matrix are an orthonomal. That is, each column is length one, and mutually perpendicular. What can we say about the rows of an orthogonal basis? Why?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AnalyticGeometry.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
