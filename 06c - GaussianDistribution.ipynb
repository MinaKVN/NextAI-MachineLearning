{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FI4gjuRVLvEK"
   },
   "source": [
    "*Credit*: some material here has been adapted from [Machine Learning: A Probabilistic Perspective](https://www.cs.ubc.ca/~murphyk/MLbook/) by Kevin P. Murphy (Chapter 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3dyf5H0Gjwul"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8XsBbx4Aa-Dl"
   },
   "source": [
    "# The Gaussian and its Longer-tailed Cousins\n",
    "\n",
    "In this notebook we will explore some common distributions for continuous-valued random variables.\n",
    "\n",
    "## Gaussian (normal) distribution\n",
    "\n",
    "The most widely used distribution in statistics and machine learning is the Gaussian or normal distribution. Its pdf is given by\n",
    "\n",
    "$$\\mathcal{N}(x|\\mu, \\sigma^2) \\triangleq \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)$$\n",
    "\n",
    "where $\\mu = \\mathbb{E}_X[x]$ is the mean (and mode), and $\\sigma^2 = \\mathbb{V}_X[x]$ is the variance. $\\frac{1}{\\sqrt{2 \\pi \\sigma^2}}$ is the normalization constant needed to ensure the density integrates to 1.\n",
    "\n",
    "We write $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ to denote $p(x)=\\mathcal{N}(x|\\mu, \\sigma^2)$. If $X \\sim \\mathcal{N}(0,1)$, we say $X$ follows a **standard normal** distribution. \n",
    "\n",
    "We will sometimes talk about the **precision** of a Gaussian, by which we mean the inverse variance: $\\lambda = 1/\\sigma^2$.\n",
    "\n",
    "The Gaussian distribution is the most widely used distribution in statistics. Why?\n",
    "\n",
    "* It has two parameters that are easy to interpret\n",
    "* The central limit theorem tells us that sums of independent random variables have an approximately Gaussian distribution, making it a good fit for modeling residual errors or \"noise\"\n",
    "* The Gaussian distribution makes the least number of assumptions (i.e. has maximum entropy) which makes it a good default choice in many cases\n",
    "* It has a simple mathematical form, which results in easy to implement, but often highly effective methods\n",
    "\n",
    "## The Student $t$ distribution\n",
    "\n",
    "One problem with the Gaussian distribution is that it is sensitive to outliers, since the log-probability only decays quadratically with distance from the centre. A more robust distribution is the **Student** $t$ **distribution**. Its pdf is as follows\n",
    "\n",
    "$$\\mathcal{T}(x|\\mu, \\sigma^2, \\nu) \\propto \\left[ 1 + \\frac{1}{\\nu} \\left( \\frac{x-\\mu}{\\sigma}\\right)^2\\right]^{-\\left(\\frac{\\nu + 1}{2}\\right)}$$\n",
    "\n",
    "where $\\mu$ is the mean, $\\sigma^2>0$ is the scale parameter, and $\\nu > 0$ is called the **degrees of freedom**.\n",
    "\n",
    "The distribution has the following properties:\n",
    "\n",
    "mean = $\\mu$, mode = $\\mu$, var = $\\frac{\\nu \\sigma^2}{(\\nu - 2)}$\n",
    "\n",
    "The variance is only defined if $\\nu > 2$. The mean is only defined if $\\nu > 1$. It is common to use $\\nu = 4$, which gives good performance in a range of problems. For $\\nu \\gg 5$, the Student distribution rapidly approaches a Gaussian distribution and loses its robustness properties.\n",
    "\n",
    "## The Laplace distribution\n",
    "\n",
    "Another distribution with heavy tails is the **Laplace distribution**, also known as the **double sided exponential** distribution. This has the following pdf:\n",
    "\n",
    "$$\\text{Lap}(x|\\mu,b) \\triangleq \\frac{1}{2b} \\exp \\left( - \\frac{|x - \\mu|}{b}\\right)$$\n",
    "\n",
    "Here $\\mu$ is a location parameter and $b>0$ is a scale parameter. This distribution has the following properties:\n",
    "\n",
    "mean = $\\mu$, mode = $\\mu$, var = $2b^2$\n",
    "\n",
    "Not only does it have heavier tails, it puts more probability density at 0 than the Gaussian. This property is a useful way to encourage sparsity in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bAD5iGeVa-D7"
   },
   "outputs": [],
   "source": [
    "# Show Gaussian, Student, Laplace pdfs and log pdfs\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=0, scale=1)\n",
    "t = lambda x : stats.t.pdf(x, df=1, loc=0, scale=1)\n",
    "l = lambda x : stats.laplace.pdf(x, loc=0, scale=1/np.sqrt(2))\n",
    "\n",
    "x = np.arange(-4, 4, 0.1)\n",
    "\n",
    "\n",
    "ax[0].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[0].plot(x, t(x), 'r.', label='Student')\n",
    "ax[0].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('pdfs')\n",
    "\n",
    "ax[1].plot(x, np.log(g(x)), 'b-', label='Gaussian')\n",
    "ax[1].plot(x, np.log(t(x)), 'r.', label='Student')\n",
    "ax[1].plot(x, np.log(l(x)), 'g--', label='Laplace')\n",
    "ax[1].set_title('log pdfs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8wlUeOGWYVbs"
   },
   "source": [
    "Let's fit each of these densities to data, with and without outliers. This should make it concrete what we mean by saying that the heavier-tailed densities are more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NPbkhboa-EC"
   },
   "outputs": [],
   "source": [
    "n = 30  # n data points\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(n)\n",
    "\n",
    "outliers = np.array([8, 8.75, 9.5])\n",
    "nn = len(outliers)\n",
    "nbins = 7\n",
    "\n",
    "# fit each of the models to the data (no outliers)\n",
    "model_g = stats.norm.fit(data)\n",
    "model_t = stats.t.fit(data)\n",
    "model_l = stats.laplace.fit(data)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, sharex=True)\n",
    "\n",
    "x = np.arange(-10, 10, 0.1)\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=model_g[0], scale=model_g[1])\n",
    "t = lambda x : stats.t.pdf(x, df=model_t[0], loc=model_t[1], scale=model_t[2])\n",
    "l = lambda x : stats.laplace.pdf(x, loc=model_l[0], scale=model_l[1])\n",
    "\n",
    "ax[0].hist(data, bins=25, range=(-10, 10),\n",
    "           density=True, alpha=0.25, facecolor='gray')\n",
    "ax[0].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[0].plot(x, t(x), 'r.', label='Student')\n",
    "ax[0].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "ax[0].legend(loc='best')\n",
    "ax[0].set_title('no outliers')\n",
    "\n",
    "# fit each of the models to the data (with outliers)\n",
    "newdata = np.r_[data, outliers]  # row concatenation\n",
    "model_g = stats.norm.fit(newdata)\n",
    "model_t = stats.t.fit(newdata)\n",
    "model_l = stats.laplace.fit(newdata)\n",
    "\n",
    "\n",
    "g = lambda x : stats.norm.pdf(x, loc=model_g[0], scale=model_g[1])\n",
    "t = lambda x : stats.t.pdf(x, df=model_t[0], loc=model_t[1], scale=model_t[2])\n",
    "l = lambda x : stats.laplace.pdf(x, loc=model_l[0], scale=model_l[1])\n",
    "\n",
    "ax[1].hist(newdata, bins=25, range=(-10, 10),\n",
    "           density=True, alpha=0.25, facecolor='gray')\n",
    "ax[1].plot(x, g(x), 'b-', label='Gaussian')\n",
    "ax[1].plot(x, t(x), 'r.', label='Student')\n",
    "ax[1].plot(x, l(x), 'g--', label='Laplace')\n",
    "\n",
    "\n",
    "ax[1].set_title('with outliers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Nym2da6AgMi"
   },
   "source": [
    "So we see that in the case where outliers in the data were present, the Gaussian really spread out its density, while the student and Laplace distributions weren't greatly affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laKE6UF8a-EG"
   },
   "source": [
    "## The multivariate Gaussian\n",
    "\n",
    "The **multivariate Gaussian** or **multivariate normal (MVN)** is the most widely used joint probability density function for continuous variables. The pdf of the MVN in $D$ dimensions is defined by the following\n",
    "\n",
    "$$\\mathcal{N}(\\mathbf{x}|\\boldsymbol\\mu,\\mathbf{\\Sigma}) \\triangleq \\frac{1}{(2 \\pi)^{D/2}|\\mathbf{\\Sigma}|^{1/2}} \\exp \\left[ - \\frac{1}{2} \\left(\\mathbf{x} - \\boldsymbol\\mu \\right)^T \\mathbf{\\Sigma}^{-1} \\left(\\mathbf{x} - \\boldsymbol\\mu\\right)\\right]$$\n",
    "\n",
    "where $\\boldsymbol\\mu = \\mathbb{E}_X[\\mathbf{x}] \\in \\mathbb{R}^D$ is the mean vector, and $\\Sigma = \\mathbb{V}_X[\\mathbf{x}]=\\text{Cov}[\\mathbf{x}, \\mathbf{x}]$ is the $D \\times D$ covariance matrix. Sometimes we will work in terms of the **precision matrix** or **concentration matrix** instead. This is just the inverse covariance matrix, $\\Lambda = \\Sigma^{-1}$. The normalization constant $(2 \\pi)^{-D/2}|\\Lambda|^{1/2}$ ensures that the pdf integrates to 1.\n",
    "\n",
    "The figure below plots some MVN densities in 2d for three different kinds of covariance matrices. A full covariance matrix has $D(D+1)/2$ parameters (we divide by 2 since $\\Sigma$ is symmetric). A diagonal covariance matrix has $D$ parameters, and has 0s on the off-diagonal terms. A **spherical** or **isotropic** covariance, $\\Sigma = \\sigma^2 \\mathbf{I}_D$, has one free parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "16nFhtC7a-EH"
   },
   "outputs": [],
   "source": [
    "# plot a MVN in 2D and 3D\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.linalg import eig, inv\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "delta = 0.05\n",
    "x = np.arange(-10.0, 10.0, delta)\n",
    "y = np.arange(-10.0, 10.0, delta)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "S = np.asarray([[2.0, 1.8],\n",
    "                [1.8, 2.0]])\n",
    "mu = np.asarray([0, 0])\n",
    "\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S[0, 0], sigmay=S[1, 1], \n",
    "                          mux=mu[0], muy=mu[1], sigmaxy=S[0, 1])\n",
    "\n",
    "#fig, ax = plt.subplots(2, 2, figsize=(10, 10),\n",
    "#                       subplot_kw={'aspect': 'equal'})\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 1)\n",
    "\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "ax.set_xlim((-6, 6))\n",
    "ax.set_ylim((-6, 6))\n",
    "ax.set_title('full')\n",
    "\n",
    "# Decorrelate\n",
    "[D, U] = eig(S)\n",
    "\n",
    "S1 = np.dot(np.dot(U.T, S), U)\n",
    "\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S1[0, 0], sigmay=S1[1, 1], \n",
    "                          mux=mu[0], muy=mu[0], sigmaxy=S1[0, 1])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 2)\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "\n",
    "ax.set_xlim((-10, 10))\n",
    "ax.set_ylim((-5, 5))\n",
    "ax.set_title('diagonal')\n",
    "\n",
    "# Whiten\n",
    "A = np.dot(np.sqrt(np.linalg.inv(np.diag(np.real(D)))), U.T)\n",
    "mu2 = np.dot(A, mu)\n",
    "S2 = np.dot(np.dot(A, S), A.T)  # may not be numerically equal to I\n",
    "\n",
    "\n",
    "#np.testing.assert_allclose(S2, np.eye(2))  # check\n",
    "print(np.allclose(S2, np.eye(2)))\n",
    "\n",
    "# plot centred on original mu, not shifted mu\n",
    "Z = mlab.bivariate_normal(X, Y, sigmax=S2[0, 0], sigmay=S2[1, 1], \n",
    "                          mux=mu[0], muy=mu[0], sigmaxy=S2[0, 1])\n",
    "\n",
    "ax = fig.add_subplot(2, 2, 3)\n",
    "CS = ax.contour(X, Y, Z)\n",
    "plt.clabel(CS, inline=1, fontsize=10)\n",
    "\n",
    "ax.set_xlim((-6, 6))\n",
    "ax.set_ylim((-6, 6))\n",
    "ax.set_title('spherical')\n",
    "\n",
    "# demonstration of how to do a surface plot\n",
    "axx = fig.add_subplot(2, 2, 4, projection='3d')\n",
    "surf = axx.plot_surface(X, Y, Z, rstride=5, cstride=5, cmap=cm.coolwarm,\n",
    "                        linewidth=0, antialiased=False)\n",
    "axx.set_title('spherical')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "GaussianDistribution.ipynb",
   "provenance": [
    {
     "file_id": "1gvHh2_ZjzTLMc1CADciI78BD_ezU3Vhd",
     "timestamp": 1568492327806
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
